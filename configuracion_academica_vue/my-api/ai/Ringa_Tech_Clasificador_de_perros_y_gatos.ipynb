{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_datasets in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.9.7)\n",
      "Requirement already satisfied: absl-py in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow_datasets) (8.1.7)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow_datasets) (0.1.8)\n",
      "Requirement already satisfied: immutabledict in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow_datasets) (4.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow_datasets) (1.26.4)\n",
      "Requirement already satisfied: promise in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow_datasets) (4.25.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\jhack alberth\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow_datasets) (6.0.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow_datasets) (18.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow_datasets) (2.32.3)\n",
      "Requirement already satisfied: simple-parsing in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow_datasets) (0.1.6)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow_datasets) (1.16.1)\n",
      "Requirement already satisfied: termcolor in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow_datasets) (2.4.0)\n",
      "Requirement already satisfied: toml in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow_datasets) (4.66.5)\n",
      "Requirement already satisfied: wrapt in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: etils>=1.9.1 in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.10.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2024.6.1)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.4.5)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (4.12.2)\n",
      "Requirement already satisfied: zipp in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\jhack alberth\\appdata\\roaming\\python\\python312\\site-packages (from click->tensorflow_datasets) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\jhack alberth\\appdata\\roaming\\python\\python312\\site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from simple-parsing->tensorflow_datasets) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in c:\\users\\jhack alberth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-metadata->tensorflow_datasets) (1.66.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "l-T47pwYw8qr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Jhack Alberth\\tensorflow_datasets\\cats_vs_dogs\\4.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d2680420624d8ea381b6f26d531764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532c6921e17d4b488a031ea1507177b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59dbc46cd0c64a5795d294b989e8f27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604ae293b3244df08c03db1cf3570444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "\"There is no item named 'PetImages\\\\\\\\Cat\\\\\\\\0.jpg' in the archive\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28msetattr\u001b[39m(tfds\u001b[38;5;241m.\u001b[39mimage_classification\u001b[38;5;241m.\u001b[39mcats_vs_dogs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_URL\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#Descargar el set de datos de perros y gatos\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m datos, metadatos \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcats_vs_dogs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_supervised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jhack Alberth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:176\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mc:\\Users\\Jhack Alberth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\load.py:661\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m`tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;124;03m    Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# fmt: skip\u001b[39;00m\n\u001b[0;32m    655\u001b[0m dbuilder \u001b[38;5;241m=\u001b[39m _fetch_builder(\n\u001b[0;32m    656\u001b[0m     name,\n\u001b[0;32m    657\u001b[0m     data_dir,\n\u001b[0;32m    658\u001b[0m     builder_kwargs,\n\u001b[0;32m    659\u001b[0m     try_gcs,\n\u001b[0;32m    660\u001b[0m )\n\u001b[1;32m--> 661\u001b[0m \u001b[43m_download_and_prepare_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    664\u001b[0m   as_dataset_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Jhack Alberth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\load.py:517\u001b[0m, in \u001b[0;36m_download_and_prepare_builder\u001b[1;34m(dbuilder, download, download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m    516\u001b[0m   download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 517\u001b[0m   \u001b[43mdbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jhack Alberth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:176\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mc:\\Users\\Jhack Alberth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:756\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_dir, download_config, file_format, permissions)\u001b[0m\n\u001b[0;32m    754\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mread_from_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir)\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 756\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[0;32m    762\u001b[0m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[0;32m    763\u001b[0m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[0;32m    764\u001b[0m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[0;32m    765\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownloaded_size\n",
      "File \u001b[1;32mc:\\Users\\Jhack Alberth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1752\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download_config\u001b[38;5;241m.\u001b[39mmax_examples_per_split \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1750\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1752\u001b[0m split_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;66;03m# Update the info object with the splits.\u001b[39;00m\n\u001b[0;32m   1755\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m splits_lib\u001b[38;5;241m.\u001b[39mSplitDict(split_infos)\n",
      "File \u001b[1;32mc:\\Users\\Jhack Alberth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1727\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._generate_splits\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1720\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m split_name, generator \u001b[38;5;129;01min\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m   1721\u001b[0m       split_generators\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1722\u001b[0m       desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating splits...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1723\u001b[0m       unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m splits\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1724\u001b[0m       leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1725\u001b[0m   ):\n\u001b[0;32m   1726\u001b[0m     filename_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_filename_template(split_name\u001b[38;5;241m=\u001b[39msplit_name)\n\u001b[1;32m-> 1727\u001b[0m     future \u001b[38;5;241m=\u001b[39m \u001b[43msplit_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit_split_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1729\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1730\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_shuffling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_shuffling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1733\u001b[0m     split_info_futures\u001b[38;5;241m.\u001b[39mappend(future)\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;66;03m# Process the result of the beam pipeline.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jhack Alberth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py:436\u001b[0m, in \u001b[0;36mSplitBuilder.submit_split_generation\u001b[1;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# Depending on the type of generator, we use the corresponding\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# `_build_from_xyz` method.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generator, Iterable):\n\u001b[1;32m--> 436\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_from_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuild_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Otherwise, beam required\u001b[39;00m\n\u001b[0;32m    438\u001b[0m   unknown_generator_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    439\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid split generator value for split `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    440\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected generator or apache_beam object. Got: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    441\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(generator)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    442\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Jhack Alberth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py:496\u001b[0m, in \u001b[0;36mSplitBuilder._build_from_generator\u001b[1;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[0;32m    486\u001b[0m serialized_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features\u001b[38;5;241m.\u001b[39mget_serialized_info()\n\u001b[0;32m    487\u001b[0m writer \u001b[38;5;241m=\u001b[39m writer_lib\u001b[38;5;241m.\u001b[39mWriter(\n\u001b[0;32m    488\u001b[0m     serializer\u001b[38;5;241m=\u001b[39mexample_serializer\u001b[38;5;241m.\u001b[39mExampleSerializer(serialized_info),\n\u001b[0;32m    489\u001b[0m     filename_template\u001b[38;5;241m=\u001b[39mfilename_template,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m     ignore_duplicates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore_duplicates,\n\u001b[0;32m    495\u001b[0m )\n\u001b[1;32m--> 496\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGenerating \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m examples...\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m examples\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_num_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mleave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmininterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jhack Alberth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jhack Alberth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Jhack Alberth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\image_classification\\cats_vs_dogs.py:117\u001b[0m, in \u001b[0;36mCatsVsDogs._generate_examples\u001b[1;34m(self, archive)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(buffer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m new_zip:\n\u001b[0;32m    116\u001b[0m   new_zip\u001b[38;5;241m.\u001b[39mwritestr(fname, img_recoded\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m--> 117\u001b[0m new_fobj \u001b[38;5;241m=\u001b[39m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m record \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: new_fobj,\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage/filename\u001b[39m\u001b[38;5;124m\"\u001b[39m: fname,\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: label,\n\u001b[0;32m    123\u001b[0m }\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m fname, record\n",
      "File \u001b[1;32mc:\\Users\\Jhack Alberth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\zipfile\\__init__.py:1604\u001b[0m, in \u001b[0;36mZipFile.open\u001b[1;34m(self, name, mode, pwd, force_zip64)\u001b[0m\n\u001b[0;32m   1601\u001b[0m     zinfo\u001b[38;5;241m.\u001b[39m_compresslevel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompresslevel\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1603\u001b[0m     \u001b[38;5;66;03m# Get info object for name\u001b[39;00m\n\u001b[1;32m-> 1604\u001b[0m     zinfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_to_write(zinfo, force_zip64\u001b[38;5;241m=\u001b[39mforce_zip64)\n",
      "File \u001b[1;32mc:\\Users\\Jhack Alberth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\zipfile\\__init__.py:1532\u001b[0m, in \u001b[0;36mZipFile.getinfo\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1530\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNameToInfo\u001b[38;5;241m.\u001b[39mget(name)\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m   1533\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThere is no item named \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m in the archive\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m info\n",
      "\u001b[1;31mKeyError\u001b[0m: \"There is no item named 'PetImages\\\\\\\\Cat\\\\\\\\0.jpg' in the archive\""
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "#Correccion temporal (22/mayo/2022)\n",
    "#Tensorflow datasets tiene error al descargar el set de perros y gatos y lo solucionaron\n",
    "#el 16 de mayo pero sigue fallando en los colabs. Entonces se agrega esta linea adicional\n",
    "#Mas detalle aqui: https://github.com/tensorflow/datasets/issues/3918\n",
    "setattr(tfds.image_classification.cats_vs_dogs, '_URL',\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\")\n",
    "\n",
    "#Descargar el set de datos de perros y gatos\n",
    "datos, metadatos = tfds.load('cats_vs_dogs', as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ux4kI-Jvxsqi"
   },
   "outputs": [],
   "source": [
    "#Imprimir los metadatos para revisarlos\n",
    "metadatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dysfe1mux5nb"
   },
   "outputs": [],
   "source": [
    "#Una forma de mostrar 5 ejemplos del set\n",
    "tfds.as_dataframe(datos['train'].take(5), metadatos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YNZO9GAyFgy"
   },
   "outputs": [],
   "source": [
    "#Otra forma de mostrar ejemplos del set\n",
    "tfds.show_examples(datos['train'], metadatos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtEYnFkNyN_s"
   },
   "outputs": [],
   "source": [
    "#Manipular y visualizar el set\n",
    "#Lo pasamos a TAMANO_IMG (100x100) y a blanco y negro (solo para visualizar)\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "TAMANO_IMG=100\n",
    "\n",
    "for i, (imagen, etiqueta) in enumerate(datos['train'].take(25)):\n",
    "  imagen = cv2.resize(imagen.numpy(), (TAMANO_IMG, TAMANO_IMG))\n",
    "  imagen = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)\n",
    "  plt.subplot(5, 5, i+1)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.imshow(imagen, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZL-LNzszqm-"
   },
   "outputs": [],
   "source": [
    "#Variable que contendra todos los pares de los datos (imagen y etiqueta) ya modificados (blanco y negro, 100x100)\n",
    "datos_entrenamiento = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niuyw2ahzwZy"
   },
   "outputs": [],
   "source": [
    "for i, (imagen, etiqueta) in enumerate(datos['train']): #Todos los datos\n",
    "  imagen = cv2.resize(imagen.numpy(), (TAMANO_IMG, TAMANO_IMG))\n",
    "  imagen = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)\n",
    "  imagen = imagen.reshape(TAMANO_IMG, TAMANO_IMG, 1) #Cambiar tamano a 100,100,1\n",
    "  datos_entrenamiento.append([imagen, etiqueta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmRCAO7V0NaF"
   },
   "outputs": [],
   "source": [
    "#Ver los datos del primer indice\n",
    "datos_entrenamiento[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBeI5BBe0Urh"
   },
   "outputs": [],
   "source": [
    "#Ver cuantos datos tengo en la variable\n",
    "len(datos_entrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9EMU7Lh0ZhJ"
   },
   "outputs": [],
   "source": [
    "#Preparar mis variables X (entradas) y y (etiquetas) separadas\n",
    "\n",
    "X = [] #imagenes de entrada (pixeles)\n",
    "y = [] #etiquetas (perro o gato)\n",
    "\n",
    "for imagen, etiqueta in datos_entrenamiento:\n",
    "  X.append(imagen)\n",
    "  y.append(etiqueta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2Y96d3A0kkI"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cN8bd2270t7A"
   },
   "outputs": [],
   "source": [
    "#Normalizar los datos de las X (imagenes). Se pasan a numero flotante y dividen entre 255 para quedar de 0-1 en lugar de 0-255\n",
    "import numpy as np\n",
    "\n",
    "X = np.array(X).astype(float) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJIGT87l03Yc"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJgbpFP-09RC"
   },
   "outputs": [],
   "source": [
    "#Convertir etiquetas en arreglo simple\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alUTJMCO1BiQ"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOEcN2B117SQ"
   },
   "outputs": [],
   "source": [
    "#Crear los modelos iniciales\n",
    "#Usan sigmoid como salida (en lugar de softmax) para mostrar como podria funcionar con dicha funcion de activacion.\n",
    "#Sigmoid regresa siempre datos entre 0 y 1. Realizamos el entrenamiento para al final considerar que si la respuesta se\n",
    "#acerca a 0, es un gato, y si se acerca a 1, es un perro.\n",
    "\n",
    "modeloDenso = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(100, 100, 1)),\n",
    "  tf.keras.layers.Dense(150, activation='relu'),\n",
    "  tf.keras.layers.Dense(150, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "modeloCNN = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(100, 100, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(100, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "modeloCNN2 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(100, 100, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "  tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(250, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eH9e4YSf24hM"
   },
   "outputs": [],
   "source": [
    "#Compilar modelos. Usar crossentropy binario ya que tenemos solo 2 opciones (perro o gato)\n",
    "modeloDenso.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "modeloCNN.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "modeloCNN2.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSbOH3Km3Goj"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icpR8Ar73NDx"
   },
   "outputs": [],
   "source": [
    "#La variable de tensorboard se envia en el arreglo de \"callbacks\" (hay otros tipos de callbacks soportados)\n",
    "#En este caso guarda datos en la carpeta indicada en cada epoca, de manera que despues\n",
    "#Tensorboard los lee para hacer graficas\n",
    "tensorboardDenso = TensorBoard(log_dir='logs/denso')\n",
    "modeloDenso.fit(X, y, batch_size=32,\n",
    "                validation_split=0.15,\n",
    "                epochs=100,\n",
    "                callbacks=[tensorboardDenso])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmRBuHwG5BOY"
   },
   "outputs": [],
   "source": [
    "#Cargar la extension de tensorboard de colab\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_xqqhin5Db_"
   },
   "outputs": [],
   "source": [
    "#Ejecutar tensorboard e indicarle que lea la carpeta \"logs\"\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QY68qkI5d5C"
   },
   "outputs": [],
   "source": [
    "tensorboardCNN = TensorBoard(log_dir='logs/cnn')\n",
    "modeloCNN.fit(X, y, batch_size=32,\n",
    "                validation_split=0.15,\n",
    "                epochs=100,\n",
    "                callbacks=[tensorboardCNN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrTNoLjPAKwK"
   },
   "outputs": [],
   "source": [
    "tensorboardCNN2 = TensorBoard(log_dir='logs/cnn2')\n",
    "modeloCNN2.fit(X, y, batch_size=32,\n",
    "                validation_split=0.15,\n",
    "                epochs=100,\n",
    "                callbacks=[tensorboardCNN2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9aiG_8jHqam"
   },
   "outputs": [],
   "source": [
    "#ver las imagenes de la variable X sin modificaciones por aumento de datos\n",
    "plt.figure(figsize=(20, 8))\n",
    "for i in range(10):\n",
    "  plt.subplot(2, 5, i+1)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.imshow(X[i].reshape(100, 100), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t25foGQbH_UD"
   },
   "outputs": [],
   "source": [
    "#Realizar el aumento de datos con varias transformaciones. Al final, graficar 10 como ejemplo\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=15,\n",
    "    zoom_range=[0.7, 1.4],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "datagen.fit(X)\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "for imagen, etiqueta in datagen.flow(X, y, batch_size=10, shuffle=False):\n",
    "  for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(imagen[i].reshape(100, 100), cmap=\"gray\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tRiHZQdJkug"
   },
   "outputs": [],
   "source": [
    "modeloDenso_AD = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(100, 100, 1)),\n",
    "  tf.keras.layers.Dense(150, activation='relu'),\n",
    "  tf.keras.layers.Dense(150, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "modeloCNN_AD = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(100, 100, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(100, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "modeloCNN2_AD = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(100, 100, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "  tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(250, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yauyCGC3JsZ7"
   },
   "outputs": [],
   "source": [
    "modeloDenso_AD.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "modeloCNN_AD.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "modeloCNN2_AD.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6K1eEKx_JyAv"
   },
   "outputs": [],
   "source": [
    "#Separar los datos de entrenamiento y los datos de pruebas en variables diferentes\n",
    "\n",
    "len(X) * .85 #19700\n",
    "len(X) - 19700 #3562\n",
    "\n",
    "X_entrenamiento = X[:19700]\n",
    "X_validacion = X[19700:]\n",
    "\n",
    "y_entrenamiento = y[:19700]\n",
    "y_validacion = y[19700:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fgSiSBwKE75"
   },
   "outputs": [],
   "source": [
    "#Usar la funcion flow del generador para crear un iterador que podamos enviar como entrenamiento a la funcion FIT del modelo\n",
    "data_gen_entrenamiento = datagen.flow(X_entrenamiento, y_entrenamiento, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WTqrVcKKMpI"
   },
   "outputs": [],
   "source": [
    "tensorboardDenso_AD = TensorBoard(log_dir='logs/denso_AD')\n",
    "\n",
    "modeloDenso_AD.fit(\n",
    "    data_gen_entrenamiento,\n",
    "    epochs=100, batch_size=32,\n",
    "    validation_data=(X_validacion, y_validacion),\n",
    "    steps_per_epoch=int(np.ceil(len(X_entrenamiento) / float(32))),\n",
    "    validation_steps=int(np.ceil(len(X_validacion) / float(32))),\n",
    "    callbacks=[tensorboardDenso_AD]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJcJ8OAw8WOJ"
   },
   "outputs": [],
   "source": [
    "tensorboardCNN_AD = TensorBoard(log_dir='logs-new/cnn_AD')\n",
    "\n",
    "modeloCNN_AD.fit(\n",
    "    data_gen_entrenamiento,\n",
    "    epochs=150, batch_size=32,\n",
    "    validation_data=(X_validacion, y_validacion),\n",
    "    steps_per_epoch=int(np.ceil(len(X_entrenamiento) / float(32))),\n",
    "    validation_steps=int(np.ceil(len(X_validacion) / float(32))),\n",
    "    callbacks=[tensorboardCNN_AD]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bwOIjLuyAsO"
   },
   "outputs": [],
   "source": [
    "tensorboardCNN2_AD = TensorBoard(log_dir='logs/cnn2_AD')\n",
    "\n",
    "modeloCNN2_AD.fit(\n",
    "    data_gen_entrenamiento,\n",
    "    epochs=100, batch_size=32,\n",
    "    validation_data=(X_validacion, y_validacion),\n",
    "    steps_per_epoch=int(np.ceil(len(X_entrenamiento) / float(32))),\n",
    "    validation_steps=int(np.ceil(len(X_validacion) / float(32))),\n",
    "    callbacks=[tensorboardCNN2_AD]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BwhiRKiAwF0"
   },
   "outputs": [],
   "source": [
    "modeloCNN_AD.save('perros-gatos-cnn-ad.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcUG8NoKA1ee"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflowjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8dSa1UzA4eW"
   },
   "outputs": [],
   "source": [
    "!mkdir carpeta_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEE6r07ZA6Mz"
   },
   "outputs": [],
   "source": [
    "!tensorflowjs_converter --input_format keras perros-gatos-cnn-ad.h5 carpeta_salida"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
